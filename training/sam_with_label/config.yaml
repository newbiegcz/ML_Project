# lightning.pytorch==2.0.2
# TODO: change monitor to other metrics

subcommand: fit
fit:
  seed_everything: 42

  trainer:
    accelerator: gpu
    strategy: auto
    devices: auto
    max_epochs: 800
    limit_train_batches: 0.02
    limit_val_batches: 0.5
    num_nodes: 1
    callbacks:
      - class_path: lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint
        init_args:
          save_top_k: 10
<<<<<<< HEAD
          monitor: val_Dice/prompt1/mDice
=======
          monitor: validation/single_point/mDice
>>>>>>> 3e78707881e6b08d61a1fc82c7ad7279eaccb3d3
          mode: max
      - class_path: lightning.pytorch.callbacks.LearningRateMonitor
        init_args:
          logging_interval: step
    overfit_batches: 0
    check_val_every_n_epoch: 1
    log_every_n_steps: 10
    accumulate_grad_batches: 1
    benchmark: null
    use_distributed_sampler: true
    profiler: null
    detect_anomaly: false
    barebones: false
    reload_dataloaders_every_n_epochs: 0
    default_root_dir: experiment_logs/lightning/
    wandb_config:
<<<<<<< HEAD
      name: task2_pass_mask
=======
      name: with_lowres_mask_prompt (Old positional embedding)
>>>>>>> 3e78707881e6b08d61a1fc82c7ad7279eaccb3d3
      project: SAM with Labels
      entity: ml-project-2023
      save_dir: experiment_logs
  model:
    model_type: vit_h
    train_image_encoder: false
    train_prompt_encoder: true
    dice_loss_coef: 0.05
    focal_loss_coef: 1
    # label_loss_coef: 0.1 # to check
    iou_loss_coef: 1.0
    # label_weight: [10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    # model_kwargs:
    #   iou_head_hidden_dim: 256
    #   label_head_hidden_dim: 256
    # prompt_3d_std: 0.2 # to check
      
    optimizer_type: AdamW
    optimizer_kwargs:
      lr: 0.0001
      weight_decay: 0.1
    dice_loss_params:
      p: 1
      smooth: 1
    focal_loss_params:
      alpha: 0.25
      gamma: 2
    debug: False
  data:
    embedding_file_path: "/root/autodl-tmp/data_with_roi/embeddings"
    datapoint_file_path: "/root/autodl-tmp/data_with_roi/datapoints"
    model_type: "vit_h"
    batch_size: 64
    num_workers: 5
<<<<<<< HEAD
    calculate_connected_mask: True
=======

    calculate_connected_mask: True

>>>>>>> 3e78707881e6b08d61a1fc82c7ad7279eaccb3d3
    debug: False
  # data:
  #     embedding_file_path: "/root/autodl-tmp/embeddings"
  #     datapoint_file_path: "/root/autodl-tmp/datapoints"
  #     model_type: "vit_h"
  #     batch_size: 64
  #     aug_per_img: 7
  #     total_aug_per_img: 10
  ckpt_path: null
