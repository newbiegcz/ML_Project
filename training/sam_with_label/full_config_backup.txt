# lightning.pytorch==2.0.2

subcommand: fit
fit:
  seed_everything: 42
  trainer:
    accelerator: auto
    strategy: auto
    devices: auto
    num_nodes: 1
    precision: 32-true
    logger:
      class_path: pytorch_lightning.loggers.wandb.WandbLogger
      init_args:
        name: debug
        project: SAM with Labels
        entity: ml-project-2023
        dir: ./wandb


    callbacks: null
    fast_dev_run: false
    max_epochs: null
    min_epochs: null
    max_steps: -1
    min_steps: null
    max_time: null
    limit_train_batches: null
    limit_val_batches: null
    limit_test_batches: null
    limit_predict_batches: null
    overfit_batches: 0.0
    val_check_interval: null
    check_val_every_n_epoch: 1
    num_sanity_val_steps: null
    log_every_n_steps: null
    enable_checkpointing: null
    enable_progress_bar: null
    enable_model_summary: null
    accumulate_grad_batches: 1
    gradient_clip_val: null
    gradient_clip_algorithm: null
    deterministic: null
    benchmark: null
    inference_mode: true
    use_distributed_sampler: true
    profiler: null
    detect_anomaly: false
    barebones: false
    plugins: null
    sync_batchnorm: false
    reload_dataloaders_every_n_epochs: 0
    default_root_dir: null
  model:
    model_type: vit_h
    pretrained_checkpoint: checkpoint/sam_vit_h_4b8939.pth
    train_image_encoder: false
    train_prompt_encoder: true
    dice_loss_coef: 1.0
    focal_loss_coef: 1.0
    label_loss_coef: 1.0
    iou_loss_coef: 1.0
    optimizer_type: AdamW
    optimizer_kwargs:
      lr: 1.0e-05
      weight_decay: 0.1
    batch_size_decoder: 1
    dice_loss_params:
      p: 1.0
      smooth: 1.0
    focal_loss_params:
      alpha: 0.25
      gamma: 2.0
  data:
    epoch_len: 1000
    chunk_size: 64
    batch_size: 64
    delay: 128
    encoder_batch_size: 2
    encoder_device: cuda:0
    cache_path: embedding_cache
    cache_size_limit: 10737418240
    seed: 1166117
    augment_training_data: true
    augment_validation_data: true
  ckpt_path: null
